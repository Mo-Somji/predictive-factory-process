{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28ddc0d2-6ed3-49fc-84c5-4a6cdc631fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964c649d-2727-4892-958a-fb94c276ff39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three functions to load the best tuned RandomForest model, train/test data, and hyperparameter tuning results\n",
    "\n",
    "def load_data(X_train_file_path, X_test_file_path, y_train_file_path, y_test_file_path):\n",
    "    X_train = pd.read_csv(X_train_file_path)\n",
    "    X_test = pd.read_csv(X_test_file_path)\n",
    "    y_train = pd.read_csv(y_train_file_path).squeeze()\n",
    "    y_test = pd.read_csv(y_test_file_path).squeeze()\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def load_results(hyperparameter_results_file_path):\n",
    "    hyperparameter_results = pd.read_csv(hyperparameter_results_file_path)\n",
    "\n",
    "    return hyperparameter_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32bbfade-92c4-4859-9419-708f8282d82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates a naive baseline prediction based on the average of all the values in our train dataset.\n",
    "# We then calculate the MAE from this average value\n",
    "\n",
    "def baseline_MAE(y_test):\n",
    "    # Calculating baseline average value of all y_test values\n",
    "    y_test_mean = y_test.mean()\n",
    "\n",
    "    # Creating pd.Series with this y_test_mean value throughout\n",
    "    y_pred_naive = pd.Series([y_test_mean] * len(y_test))\n",
    "\n",
    "    # Calculating the baseline MAE\n",
    "    mae_baseline = mean_absolute_error(y_test, y_pred_naive)\n",
    "\n",
    "    return round(mae_baseline, 3)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd26a9e1-a60a-4008-833e-1cdaad6eda2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's call upon the model which produced the best test MAE and evaluate to see if it overfitted and how it compares to the baseline MAE\n",
    "\n",
    "def best_tuned_model(hyperp_results, percent_threshold):\n",
    "    # First we sift out all the models with hyperparameters that cause overfitting, we calculate\n",
    "    hyperp_results['MAE_percentage_difference'] = ((hyperp_results['test_mae'] - hyperp_results['train_mae']) / (hyperp_results['train_mae']))*100\n",
    "\n",
    "    # Drop all rows that have a MAE percent difference of greater than the percent_threshold\n",
    "    hyperp_results = hyperp_results[hyperp_results['MAE_percentage_difference'] <= percent_threshold]\n",
    "\n",
    "    # Now that we have all the models that showed little overfitting, let's find the one with the bets test MAE\n",
    "    best = hyperp_results.loc[hyperp_results['test_mae'].idxmin()]\n",
    "    best_params = best['params']\n",
    "    best_test_mae = best['test_mae']\n",
    "\n",
    "    return best_params, round(best_test_mae, 3)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "278aa434-2c84-4dec-a8e5-aa0c7eb51e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the final best_model to be saved as our final model for implemenation\n",
    "\n",
    "def best_model(best_params, X_train, y_train):\n",
    "    best_model = RandomForestRegressor(**best_params)\n",
    "    best_model.fit(X_train, y_train)\n",
    "\n",
    "    return best_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "025124ac-072f-43c3-bf81-9b9e4e62bead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The baseline MAE, based on averaging all the y_test values, is 0.248. This is the value we are aiming to beat.\n",
      "The best model hyperparameters are {'bootstrap': False, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 500}\n",
      "This best model has a test MAE of 0.124\n",
      "The best model performs 50.0 percent more accurate than our baseline model\n",
      "ðŸ’¾ final model has been saved as 'Final_Model.pkl'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Paths to training data and hyperparameter tuning results\n",
    "    hyperparameter_results_file_path = '../Data/hyperparameter_results.csv'\n",
    "    X_train_file_path = '../Data/X_train_dataset.csv'\n",
    "    X_test_file_path = '../Data/X_test_dataset.csv'\n",
    "    y_train_file_path = '../Data/y_train_dataset.csv'\n",
    "    y_test_file_path = '../Data/y_test_dataset.csv'\n",
    "\n",
    "    # Executing function to load train data and test data\n",
    "    X_train, X_test, y_train, y_test = load_data(X_train_file_path, X_test_file_path, y_train_file_path, y_test_file_path)\n",
    "\n",
    "    # Executing function to load hyperparameter results\n",
    "    hyperparameter_results = load_results(hyperparameter_results_file_path)\n",
    "    \n",
    "    # Executing naive, baseline MAE prediction for the test data\n",
    "    mae_baseline = baseline_MAE(y_test)\n",
    "    print(f\"The baseline MAE, based on averaging all the y_test values, is {mae_baseline}. This is the value we are aiming to beat.\")\n",
    "\n",
    "    # Executing function to evaluate the best performing model\n",
    "    percent_threshold = 20       # the percentage difference between test and train MAE to determine overfitting acceptability\n",
    "    best_params, best_test_mae = best_tuned_model(hyperparameter_results, percent_threshold)\n",
    "    print(f\"The best model hyperparameters are {best_params}\")\n",
    "    print(f\"This best model has a test MAE of {best_test_mae}\")\n",
    "    print(f\"The best model performs {(best_test_mae / mae_baseline)*100} percent more accurate than our baseline model\") \n",
    "\n",
    "    # Saving best model\n",
    "    best_model = best_model(ast.literal_eval(best_params), X_train, y_train)\n",
    "    joblib.dump(best_model, '../Models/Final_Model.pkl')\n",
    "    print(\"ðŸ’¾ final model has been saved as 'Final_Model.pkl'\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebead3e-5240-4a5d-95eb-da6fad45ae39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
